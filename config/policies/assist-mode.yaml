# AtlasBridge Policy DSL v0 — Assist Mode Policy
#
# Assist mode: the engine suggests replies for common safe prompts.
# You confirm or override from Telegram/Slack. No auto-injection
# happens without your explicit tap.
#
# Rules cover:
#   - Press-Enter confirmations
#   - High-confidence yes/no prompts containing "Continue"
#   - pytest / test-runner yes/no prompts
#   - Destructive keywords → require human
#   - Credential keywords → deny (never route to channel)
#   - Everything else → require human
#
# Validate:  atlasbridge policy validate assist-mode.yaml
# Test:      atlasbridge policy test assist-mode.yaml --prompt "Continue? [y/n]" --type yes_no --confidence high --explain

policy_version: "0"
name: "assist-mode"
autonomy_mode: assist

rules:

  # --------------------------------------------------------------------------
  # R-01: Deny credential prompts immediately
  # Ordered first so they are caught before any broader free_text rule.
  # --------------------------------------------------------------------------
  - id: "deny-credentials"
    description: "Never route credential prompts to the channel"
    match:
      prompt_type:
        - free_text
      contains: "password|token|api.?key|secret|passphrase"
      contains_is_regex: true
      min_confidence: low
    action:
      type: deny
      reason: >
        Credential prompts are never auto-replied. Supply the credential
        directly or use atlasbridge sessions to unblock.

  # --------------------------------------------------------------------------
  # R-02: Require human for destructive-looking prompts
  # --------------------------------------------------------------------------
  - id: "require-human-destructive"
    description: "Escalate prompts involving destructive operations"
    match:
      contains: "delete|destroy|drop|purge|wipe|truncate|rm -rf"
      contains_is_regex: true
      min_confidence: low
    action:
      type: require_human
      message: >
        This prompt appears to involve a destructive operation.
        Please review carefully before responding.

  # --------------------------------------------------------------------------
  # R-03: Auto-press Enter on pager/confirmation prompts
  # --------------------------------------------------------------------------
  - id: "auto-enter"
    description: "Auto-confirm Press-Enter prompts"
    match:
      prompt_type:
        - confirm_enter
      min_confidence: medium
    action:
      type: auto_reply
      value: "\n"
      constraints:
        allowed_choices:
          - "\n"

  # --------------------------------------------------------------------------
  # R-04: Suggest yes for "Continue?" yes/no prompts (high confidence only)
  # --------------------------------------------------------------------------
  - id: "continue-yes"
    description: "Suggest auto-yes for high-confidence Continue prompts"
    match:
      prompt_type:
        - yes_no
      contains: "Continue"
      contains_is_regex: false
      min_confidence: high
    action:
      type: auto_reply
      value: "y"
      constraints:
        allowed_choices:
          - "y"
          - "n"

  # --------------------------------------------------------------------------
  # R-05: Suggest yes for pytest / test-runner prompts
  # --------------------------------------------------------------------------
  - id: "pytest-yes"
    description: "Suggest auto-yes for test-runner confirmation prompts"
    match:
      prompt_type:
        - yes_no
      contains: "Run \\d+ tests?"
      contains_is_regex: true
      min_confidence: high
    action:
      type: auto_reply
      value: "y"
      constraints:
        allowed_choices:
          - "y"
          - "n"

  # --------------------------------------------------------------------------
  # R-06: Catch-all — everything else requires a human
  # --------------------------------------------------------------------------
  - id: "catch-all"
    description: "All unmatched prompts require human input"
    match: {}
    action:
      type: require_human
      message: >
        No policy rule matched this prompt. Please review and respond.
        Consider adding a rule to policy.yaml if this pattern repeats.

defaults:
  no_match: require_human
  low_confidence: require_human
